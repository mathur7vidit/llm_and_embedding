{
 "cells": [
  {
   "cell_type": "code",
   "id": "96d8eaeb0d8eb3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:41:54.427457Z",
     "start_time": "2026-02-04T17:41:33.765592Z"
    }
   },
   "source": [
    "# Summarization - Test Code 1\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import os\n",
    "import re\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# -----------------------------\n",
    "# SUMMARIZATION TASK (≤ 30 WORDS, CLEAN ENDING)\n",
    "# -----------------------------\n",
    "\n",
    "text = \"\"\"Summarize in 30 words:\n",
    "AI is transforming industries by bringing them to the next level.\n",
    "The company is building its brand and investing in innovative technologies\n",
    "that make the automotive industry more efficient and sustainable.\n",
    "It is also developing new approaches focused on creating a cleaner and more sustainable future.\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,\n",
    ")\n",
    "\n",
    "# Step 1: Generate summary\n",
    "raw_output = summarizer(\n",
    "    text,\n",
    "    max_length=60,\n",
    "    min_length=10,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "summary = raw_output[0][\"summary_text\"].strip()\n",
    "\n",
    "# ---- Step 2: Split into clean sentences ----\n",
    "sentences = re.split(r'(?<=[.!?]) +', summary)\n",
    "\n",
    "# ---- Step 3: Rebuild summary to <= 30 words ----\n",
    "final_sentences = []\n",
    "word_count = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_words = len(sentence.split())\n",
    "\n",
    "    if word_count + sentence_words <= 30:\n",
    "        final_sentences.append(sentence)\n",
    "        word_count += sentence_words\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# ---- Step 4: Fallback: if no full sentence fits ----\n",
    "if not final_sentences:\n",
    "    words = summary.split()[:30]\n",
    "    cleaned = \" \".join(words).rstrip(\",\")\n",
    "    summary_final = cleaned + \"...\"\n",
    "else:\n",
    "    summary_final = \" \".join(final_sentences)\n",
    "\n",
    "# Output\n",
    "print(\"\\n===== CLEAN SUMMARY (≤ 30 WORDS) =====\")\n",
    "print(summary_final)\n",
    "print(f\"\\nWord count: {len(summary_final.split())}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CLEAN SUMMARY (≤ 30 WORDS) =====\n",
      "The company is building its brand and investing in innovative technologiesthat make the automotive industry more efficient and sustainable.\n",
      "\n",
      "Word count: 19\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "40f84b65252b6778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:42:11.487454Z",
     "start_time": "2026-02-04T17:42:02.802395Z"
    }
   },
   "source": [
    "# Summarization - Test Code 2\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import os\n",
    "import re\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,\n",
    ")\n",
    "\n",
    "text = \"\"\"Summarize in 30 words:\n",
    "AI is transforming industries by bringing them to the next level. The company is building its brand and investing in innovative technologies that make the automotive industry more efficient and sustainable.\n",
    "It is also developing new approaches focused on creating a cleaner and more sustainable future.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Generate summary\n",
    "raw_summary = generator(\n",
    "    f\"Summarize this in 30 words or fewer:\\n{text}\",\n",
    "    max_length=60,\n",
    "    min_length=10,\n",
    "    do_sample=False\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "# Step 2: Split into sentences\n",
    "sentences = re.split(r\"(?<=[.!?]) +\", raw_summary.strip())\n",
    "\n",
    "# Step 3: Rebuild using full sentences up to 30 words\n",
    "final_sentences = []\n",
    "word_count = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    if word_count + len(words) <= 30:\n",
    "        final_sentences.append(sentence)\n",
    "        word_count += len(words)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Step 4: Fallback if first sentence > 30 words\n",
    "if not final_sentences:\n",
    "    trimmed = \" \".join(raw_summary.split()[:30]).rstrip(\",\")\n",
    "    clean_summary = trimmed + \"...\"\n",
    "else:\n",
    "    clean_summary = \" \".join(final_sentences)\n",
    "\n",
    "print(\"\\n===== CLEAN SUMMARY (≤ 30 WORDS) =====\")\n",
    "print(clean_summary)\n",
    "print(\"Word count:\", len(clean_summary.split()))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CLEAN SUMMARY (≤ 30 WORDS) =====\n",
      "AI is transforming industries by bringing them to the next level. The company is building its brand and investing in innovative technologies.\n",
      "Word count: 22\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ef6f2b0262dad304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:45:35.779925Z",
     "start_time": "2026-02-04T17:43:56.620429Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "# prompt = \"Give me a short introduction to large language model.\"\n",
    "prompt = \"who was the first person to land on moon\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "# thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "# print(\"thinking content:\", thinking_content)\n",
    "print(\"answer:\", content)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: The first person to land on the Moon was **Neil Armstrong**. He was part of the Apollo 17 mission, which successfully landed him on the lunar surface in 1969. He is widely recognized as the first human to walk on the Moon.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "6e5f0974500a33cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:50:16.021461Z",
     "start_time": "2026-02-04T17:49:49.888501Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Ensure padding token is set\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def generate_ai_poem():\n",
    "    # Stronger poetic instruction + style example\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Write a clean, lyrical 4-line poem about artificial intelligence and imagination.\\n\"\n",
    "                \"Follow best poetic practices: no bullet points, no dashes, no numbering.\\n\"\n",
    "                \"Each line should be a complete poetic sentence.\\n\"\n",
    "                \"Example style:\\n\"\n",
    "                \"Dreams unfold in quiet threads of light,\\n\"\n",
    "                \"Thoughts take flight through endless night,\\n\"\n",
    "                \"Whispers bloom where ideas rise,\\n\"\n",
    "                \"A gentle spark beneath the skies.\\n\\n\"\n",
    "                \"Now write a new poem in a similar tone.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Use Qwen chat template without chain-of-thought\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer([chat_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            top_k=30,\n",
    "            pad_token_id=model.config.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only newly generated part\n",
    "    new_tokens = generated[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    raw_output = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # --- CLEANUP PIPELINE ---\n",
    "    # Remove bullet points, hyphens, numbers, weird symbols\n",
    "    cleaned = re.sub(r\"^[\\-\\•\\*\\d\\.]+\\s*\", \"\", raw_output, flags=re.MULTILINE)\n",
    "\n",
    "    # Split into non-empty lines\n",
    "    lines = [line.strip() for line in cleaned.splitlines() if line.strip()]\n",
    "\n",
    "    # Keep exactly 4 lines\n",
    "    lines = lines[:4]\n",
    "\n",
    "    # Capitalize first letter if missing (optional best practice)\n",
    "    lines = [line[0].upper() + line[1:] if line and line[0].islower() else line for line in lines]\n",
    "\n",
    "    poem = \"\\n\".join(lines)\n",
    "    return poem\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(generate_ai_poem())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The machine dreams with no name,\n",
      "Its mind a symphony of code.\n",
      "Imagination breathes in its soul,\n",
      "And dreams come from logic's core.\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
