{
 "cells": [
  {
   "cell_type": "code",
   "id": "96d8eaeb0d8eb3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:43:23.107173Z",
     "start_time": "2025-12-15T18:43:09.560355Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import os\n",
    "import re\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# -----------------------------\n",
    "# SUMMARIZATION TASK (≤ 30 WORDS, CLEAN ENDING)\n",
    "# -----------------------------\n",
    "\n",
    "text = \"\"\"Summarize in 30 words:\n",
    "AI is transforming industries by bringing them to the next level.\n",
    "The company is building its brand and investing in innovative technologies\n",
    "that make the automotive industry more efficient and sustainable.\n",
    "It is also developing new approaches focused on creating a cleaner and more sustainable future.\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,\n",
    ")\n",
    "\n",
    "# Step 1: Generate summary\n",
    "raw_output = summarizer(\n",
    "    text,\n",
    "    max_length=60,\n",
    "    min_length=10,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "summary = raw_output[0][\"summary_text\"].strip()\n",
    "\n",
    "# ---- Step 2: Split into clean sentences ----\n",
    "sentences = re.split(r'(?<=[.!?]) +', summary)\n",
    "\n",
    "# ---- Step 3: Rebuild summary to <= 30 words ----\n",
    "final_sentences = []\n",
    "word_count = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_words = len(sentence.split())\n",
    "\n",
    "    if word_count + sentence_words <= 30:\n",
    "        final_sentences.append(sentence)\n",
    "        word_count += sentence_words\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# ---- Step 4: Fallback: if no full sentence fits ----\n",
    "if not final_sentences:\n",
    "    words = summary.split()[:30]\n",
    "    cleaned = \" \".join(words).rstrip(\",\")\n",
    "    summary_final = cleaned + \"...\"\n",
    "else:\n",
    "    summary_final = \" \".join(final_sentences)\n",
    "\n",
    "# Output\n",
    "print(\"\\n===== CLEAN SUMMARY (≤ 30 WORDS) =====\")\n",
    "print(summary_final)\n",
    "print(f\"\\nWord count: {len(summary_final.split())}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CLEAN SUMMARY (≤ 30 WORDS) =====\n",
      "The company is building its brand and investing in innovative technologiesthat make the automotive industry more efficient and sustainable.\n",
      "\n",
      "Word count: 19\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "40f84b65252b6778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:44:16.138037Z",
     "start_time": "2025-12-15T18:44:03.945254Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import os\n",
    "import re\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# MODEL LOADING (Shared for Summarization + QA)\n",
    "# -----------------------------------------\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# CASE 1: SUMMARIZATION (≤ 30 words, with clean sentence end)\n",
    "# ===========================================================\n",
    "\n",
    "text = \"\"\"Summarize in 30 words:\n",
    "AI is transforming industries by bringing them to the next level. The company is building its brand and investing in innovative technologies that make the automotive industry more efficient and sustainable.\n",
    "It is also developing new approaches focused on creating a cleaner and more sustainable future.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Generate summary\n",
    "raw_summary = generator(\n",
    "    f\"Summarize this in 30 words or fewer:\\n{text}\",\n",
    "    max_length=60,\n",
    "    min_length=10,\n",
    "    do_sample=False\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "# Step 2: Split into sentences\n",
    "sentences = re.split(r\"(?<=[.!?]) +\", raw_summary.strip())\n",
    "\n",
    "# Step 3: Rebuild using full sentences up to 30 words\n",
    "final_sentences = []\n",
    "word_count = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    if word_count + len(words) <= 30:\n",
    "        final_sentences.append(sentence)\n",
    "        word_count += len(words)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Step 4: Fallback if first sentence > 30 words\n",
    "if not final_sentences:\n",
    "    trimmed = \" \".join(raw_summary.split()[:30]).rstrip(\",\")\n",
    "    clean_summary = trimmed + \"...\"\n",
    "else:\n",
    "    clean_summary = \" \".join(final_sentences)\n",
    "\n",
    "print(\"\\n===== CLEAN SUMMARY (≤ 30 WORDS) =====\")\n",
    "print(clean_summary)\n",
    "print(\"Word count:\", len(clean_summary.split()))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CLEAN SUMMARY (≤ 30 WORDS) =====\n",
      "AI is transforming industries by bringing them to the next level. The company is building its brand and investing in innovative technologies.\n",
      "Word count: 22\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ef6f2b0262dad304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:46:16.773504Z",
     "start_time": "2025-12-15T18:44:20.373429Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "# prompt = \"Give me a short introduction to large language model.\"\n",
    "prompt = \"who was the first person to land on moon\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "# thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "# print(\"thinking content:\", thinking_content)\n",
    "print(\"answer:\", content)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: The first person to land on the Moon was **Neil Armstrong**, who was part of the Apollo 17 mission (1972) aboard the **Command Module Columbia**. He became the first human to walk on the Moon.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "44e2735e51243f9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:47:40.840567Z",
     "start_time": "2025-12-15T18:46:30.356812Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "# Prompt for a creative poem\n",
    "prompt = \"Write a 4-line poem about artificial intelligence and imagination.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate poem\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=80,\n",
    "    temperature=0.9,     # more creative\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a 4-line poem about artificial intelligence and imagination. Use a free verse. The lines must have a rhyme scheme that follows the pattern AABBCC. The first and third lines must be a line of free verse, and the second and fourth lines must be a line of free verse. The poem should be in English. Also, make sure that the poem is about AI and imagination. Use vivid imagery and metaphors. Avoid using any markdown.\n",
      "Okay,\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "e7f47705e07a8511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:48:00.825691Z",
     "start_time": "2025-12-15T18:47:45.702194Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"openai-community/gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "prompt = (\n",
    "    \"Write a simple and lyrical 4-line poem about artificial intelligence and imagination:\\n\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Remove the prompt and extract only the poem\n",
    "poem = generated[len(prompt):].strip()\n",
    "\n",
    "print(poem)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can write a simple and lyrical 4-line poem about artificial intelligence and imagination:\n",
      "\n",
      "And you can write a simple and lyrical 4-line poem about artificial intelligence and imagination:\n",
      "\n",
      "In the next paragraph you can write a simple and lyrical 4-line poem about artificial\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "6e5f0974500a33cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:50:13.280400Z",
     "start_time": "2025-12-15T18:49:53.524790Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Ensure padding token is set\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def generate_ai_poem():\n",
    "    # Stronger poetic instruction + style example\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Write a clean, lyrical 4-line poem about artificial intelligence and imagination.\\n\"\n",
    "                \"Follow best poetic practices: no bullet points, no dashes, no numbering.\\n\"\n",
    "                \"Each line should be a complete poetic sentence.\\n\"\n",
    "                \"Example style:\\n\"\n",
    "                \"Dreams unfold in quiet threads of light,\\n\"\n",
    "                \"Thoughts take flight through endless night,\\n\"\n",
    "                \"Whispers bloom where ideas rise,\\n\"\n",
    "                \"A gentle spark beneath the skies.\\n\\n\"\n",
    "                \"Now write a new poem in a similar tone.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Use Qwen chat template without chain-of-thought\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer([chat_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            top_k=30,\n",
    "            pad_token_id=model.config.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only newly generated part\n",
    "    new_tokens = generated[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    raw_output = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # --- CLEANUP PIPELINE ---\n",
    "    # Remove bullet points, hyphens, numbers, weird symbols\n",
    "    cleaned = re.sub(r\"^[\\-\\•\\*\\d\\.]+\\s*\", \"\", raw_output, flags=re.MULTILINE)\n",
    "\n",
    "    # Split into non-empty lines\n",
    "    lines = [line.strip() for line in cleaned.splitlines() if line.strip()]\n",
    "\n",
    "    # Keep exactly 4 lines\n",
    "    lines = lines[:4]\n",
    "\n",
    "    # Capitalize first letter if missing (optional best practice)\n",
    "    lines = [line[0].upper() + line[1:] if line and line[0].islower() else line for line in lines]\n",
    "\n",
    "    poem = \"\\n\".join(lines)\n",
    "    return poem\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(generate_ai_poem())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In circuits where logic bends,\n",
      "Imagination dances on the mind.\n",
      "Guided by code and thought,\n",
      "Creativity breathes in the dark.\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
