{
 "cells": [
  {
   "cell_type": "code",
   "id": "9fe4c62813786a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:40:07.858396Z",
     "start_time": "2026-02-04T17:39:48.103800Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, pipeline, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"openai-community/gpt2\"\n",
    "# model_name = \"distilbert/distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,\n",
    ")\n",
    "\n",
    "prompt = \"AI is transforming industries by\"\n",
    "\n",
    "\n",
    "def clean_to_last_sentence(full_text: str, prompt: str) -> str:\n",
    "    # Remove the prompt from the beginning (handle case where prompt might not match exactly)\n",
    "    if full_text.startswith(prompt):\n",
    "        gen_part = full_text[len(prompt):].strip()\n",
    "    else:\n",
    "        # If prompt doesn't match, try to find it\n",
    "        idx = full_text.find(prompt)\n",
    "        if idx != -1:\n",
    "            gen_part = full_text[idx + len(prompt):].strip()\n",
    "        else:\n",
    "            gen_part = full_text.strip()\n",
    "\n",
    "    # find last sentence-ending punctuation\n",
    "    last_idx = -1\n",
    "    for ch in [\".\", \"!\", \"?\"]:\n",
    "        idx = gen_part.rfind(ch)\n",
    "        if idx > last_idx:\n",
    "            last_idx = idx\n",
    "\n",
    "    if last_idx != -1:\n",
    "        gen_part = gen_part[: last_idx + 1]\n",
    "    else:\n",
    "        # fallback: cut at last space so it doesn't end mid-word\n",
    "        last_space = gen_part.rfind(\" \")\n",
    "        if last_space != -1:\n",
    "            gen_part = gen_part[: last_space]\n",
    "\n",
    "    return (prompt.strip() + \" \" + gen_part).strip()\n",
    "\n",
    "\n",
    "outputs = generator(\n",
    "    prompt + \" \",\n",
    "    max_new_tokens=100,      # more room to finish\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "for i, out in enumerate(outputs, 1):\n",
    "    raw = out[\"generated_text\"]\n",
    "    # Fix: account for the space added to prompt\n",
    "    cleaned = clean_to_last_sentence(raw, prompt + \" \")\n",
    "    # print(f\"\\n=== Completion {i} ===\")\n",
    "    # print(raw)\n",
    "    print(f\"\\n=== Completion {i} (cleaned) ===\")\n",
    "    print(cleaned)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Completion 1 (cleaned) ===\n",
      "AI is transforming industries by treating them with the same care as the rest of the population.\n",
      "In my book, I've discussed how we can increase productivity by using technology to change the way we live and work.  It's time to get back to that.  I have some interesting ideas for you.\n",
      "In the beginning, I used to think that we should focus on technology.  I thought we needed to focus on the people and how they interacted with technology.\n",
      "\n",
      "=== Completion 2 (cleaned) ===\n",
      "AI is transforming industries by vernacularizing and reinventing the way we think and act.\n",
      "\n",
      "In this year's edition of the American Economic Review, I'll talk about the latest in the \"Big Five\" research on the value of innovation. It's a fascinating look at how those big five groups are shaping the way we think, act and live.\n",
      "\n",
      "=== Completion 3 (cleaned) ===\n",
      "AI is transforming industries by creating new jobs and creating new revenue streams, while also bringing about a return to a world of \"zero-sum\" conflicts.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a8677957d73c335f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T17:40:17.595570Z",
     "start_time": "2026-02-04T17:40:17.578335Z"
    }
   },
   "source": [
    "sentence = \"LLMs are powerful tools for natural language understanding.\"\n",
    "\n",
    "# Tokenise\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "encoded = tokenizer(sentence)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"\\nToken IDs:\", token_ids)\n",
    "print(\"\\nSequence Length:\", len(token_ids))\n",
    "print(\"\\nFull Encoded Output:\", encoded)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['LL', 'Ms', 'Ġare', 'Ġpowerful', 'Ġtools', 'Ġfor', 'Ġnatural', 'Ġlanguage', 'Ġunderstanding', '.']\n",
      "\n",
      "Token IDs: [3069, 10128, 389, 3665, 4899, 329, 3288, 3303, 4547, 13]\n",
      "\n",
      "Sequence Length: 10\n",
      "\n",
      "Full Encoded Output: {'input_ids': [3069, 10128, 389, 3665, 4899, 329, 3288, 3303, 4547, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
