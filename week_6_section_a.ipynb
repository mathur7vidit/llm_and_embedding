{
 "cells": [
  {
   "cell_type": "code",
   "id": "9fe4c62813786a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:42:54.134489Z",
     "start_time": "2025-12-15T18:42:45.640562Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, pipeline, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"openai-community/gpt2\"\n",
    "# model_name = \"distilbert/distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,\n",
    ")\n",
    "\n",
    "prompt = \"AI is transforming industries by\"\n",
    "\n",
    "\n",
    "def clean_to_last_sentence(full_text: str, prompt: str) -> str:\n",
    "    # Remove the prompt from the beginning (handle case where prompt might not match exactly)\n",
    "    if full_text.startswith(prompt):\n",
    "        gen_part = full_text[len(prompt):].strip()\n",
    "    else:\n",
    "        # If prompt doesn't match, try to find it\n",
    "        idx = full_text.find(prompt)\n",
    "        if idx != -1:\n",
    "            gen_part = full_text[idx + len(prompt):].strip()\n",
    "        else:\n",
    "            gen_part = full_text.strip()\n",
    "\n",
    "    # find last sentence-ending punctuation\n",
    "    last_idx = -1\n",
    "    for ch in [\".\", \"!\", \"?\"]:\n",
    "        idx = gen_part.rfind(ch)\n",
    "        if idx > last_idx:\n",
    "            last_idx = idx\n",
    "\n",
    "    if last_idx != -1:\n",
    "        gen_part = gen_part[: last_idx + 1]\n",
    "    else:\n",
    "        # fallback: cut at last space so it doesn't end mid-word\n",
    "        last_space = gen_part.rfind(\" \")\n",
    "        if last_space != -1:\n",
    "            gen_part = gen_part[: last_space]\n",
    "\n",
    "    return (prompt.strip() + \" \" + gen_part).strip()\n",
    "\n",
    "\n",
    "outputs = generator(\n",
    "    prompt + \" \",\n",
    "    max_new_tokens=100,      # more room to finish\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "for i, out in enumerate(outputs, 1):\n",
    "    raw = out[\"generated_text\"]\n",
    "    # Fix: account for the space added to prompt\n",
    "    cleaned = clean_to_last_sentence(raw, prompt + \" \")\n",
    "    # print(f\"\\n=== Completion {i} ===\")\n",
    "    # print(raw)\n",
    "    print(f\"\\n=== Completion {i} (cleaned) ===\")\n",
    "    print(cleaned)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Completion 1 (cleaned) ===\n",
      "AI is transforming industries by focusing on new technology.\n",
      "The new tech will help transform our lives, our economy, and our lives around the world.\n",
      "I've been a bit of a skeptic, but I was excited to hear about the big tech companies.\n",
      "In the past few years, I've been learning about the emerging technologies that have made their way into our lives. They're like the Google Glass that's out there, the smart speaker that you hear every day.\n",
      "\n",
      "=== Completion 2 (cleaned) ===\n",
      "AI is transforming industries by making sure they have a better supply chain, and more efficient and cost-effective ways to sell products, and by making sure they are environmentally sound.\n",
      "I've been involved in a lot of things for the past ten years, and I am very happy with the way things are moving forward. I'm a big fan of the new products that are being made here, and I think that the industry is very well positioned to do well in this new environment.\n",
      "\n",
      "=== Completion 3 (cleaned) ===\n",
      "AI is transforming industries by developing new technologies that can revolutionize the way we think about what it means to be a human being.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "a8677957d73c335f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:43:04.206945Z",
     "start_time": "2025-12-15T18:43:04.194143Z"
    }
   },
   "source": [
    "sentence = \"LLMs are powerful tools for natural language understanding.\"\n",
    "\n",
    "# Tokenise\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "encoded = tokenizer(sentence)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"\\nToken IDs:\", token_ids)\n",
    "print(\"\\nSequence Length:\", len(token_ids))\n",
    "print(\"\\nFull Encoded Output:\", encoded)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['LL', 'Ms', 'Ġare', 'Ġpowerful', 'Ġtools', 'Ġfor', 'Ġnatural', 'Ġlanguage', 'Ġunderstanding', '.']\n",
      "\n",
      "Token IDs: [3069, 10128, 389, 3665, 4899, 329, 3288, 3303, 4547, 13]\n",
      "\n",
      "Sequence Length: 10\n",
      "\n",
      "Full Encoded Output: {'input_ids': [3069, 10128, 389, 3665, 4899, 329, 3288, 3303, 4547, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
